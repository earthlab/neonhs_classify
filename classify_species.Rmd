---
title: "Classify veg"
author: "Victoria Scholl"
date: "08/20/2020 Earth Lab GRA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidyr)
library(dplyr)
library(randomForest)
library(caret)
library(rfPermute)
library(parallel)
library(doParallel)

dir.create("output")
```

## Data preparation

```{r read_data, message=FALSE}
# start timer
start_time <- Sys.time()

# read the cleaned data, labeled for train, test, 
cleaned_data <- read_csv(here::here("data","cleaned_spectra.csv"))
```

Format the data so each row is a sample (reflectance spectrum) and each column is a descriptive feature (reflectance value at each wavelength). Each row must have a label to predict for the classification, such as genusSpecies (the first two pieces of each scientific name excluding any variety info. Example: Pseudotsuga menziesii. Genus is Pseudotsuga, species is menziesii).

```{r format_data}
# columns to keep for training the classifier
cols_to_keep <- c("spectraID", "band_idx", "reflectance", "genusSpecies")

# training set -------------------------------------------------
# select and format the training data 
train_data <- cleaned_data %>% 
  # keep only the spectra marked as part of the "train" or "test" set
    # train 60% + test 20% = 80% for training with k-fold cross validation
  dplyr::filter(group == "train" | group == "test") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # filter spectra identified at the genus level, indicated by species == "sp."
  dplyr::filter(species != "sp.") %>% 
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 
  # VS-NOTE: subset small # of samples for testing!!!
  #dplyr::sample_frac(0.5)

# convert genusSpecies to factor, so R does classification instead of regression.
# reassign the factor levels after subsetting the training data
train_data$genusSpecies <- factor(train_data$genusSpecies)



# validation set -----------------------------------------------
valid_data <- cleaned_data %>% 
  #  keep only the spectra marked as part of the independent validation set
  dplyr::filter(group == "valid") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # filter spectra identified at the genus level, indicated by species == "sp."
  dplyr::filter(species != "sp.") %>%
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 

valid_data$genusSpecies <- factor(valid_data$genusSpecies)
```


## Parameter tuning

Two hyperparameters to tune in randomForest classification: mtry and ntree

```{r caret_parallel, eval=FALSE}
start.time = Sys.time()

cl <- makePSOCKcluster(parallel::detectCores() -1) 

registerDoParallel(cl)

## All subsequent models are then run in parallel
model <- train(factor(genusSpecies) ~ ., 
               data = train_data, 
               tuneGrid = expand.grid(mtry = seq(from = 6, to = 30, by = 3)),
               method = "rf",
               metric = "Accuracy")

print(model)

## When you are done:
stopCluster(cl)

end.time = Sys.time()

print(end.time - start.time)

# took 18 minutes to test 8 values of mtry with 60% subset of training data

# save the output model to file 
saveRDS(model, "output/model_mtry_tuning.rds")
```

```{r plot_mtry_tuning}
# load mtry tuning model
model <- readRDS("output/model_mtry_tuning.rds")

# save plot to file
pdf("output/mtry_tuning_plot.pdf",
    width = 6,
    height = 4)

plot(model)

dev.off()

plot(model)
```

```{r optimize_ntree, eval=FALSE}
# https://rpubs.com/phamdinhkhanh/389752

start.time = Sys.time()

set.seed(44)

# start parallel cluster
cl <- makePSOCKcluster(parallel::detectCores() -1) 
registerDoParallel(cl)


# iteratively check ntree values 
modellist <- list()
for (ntree in c(100, 200, 500, 1000, 2000)){
  print(ntree)
  fit <- train(factor(genusSpecies) ~ ., 
                data = train_data, 
                method = "rf",
                metric = "Accuracy",
                ntree = ntree,
                tuneGrid=expand.grid(mtry = 27), 
                verbose = TRUE)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

# when you are done with parallel
stopCluster(cl)

# save model list to file 
saveRDS(modellist, "output/modelList_ntree_tuning.rds")

end.time = Sys.time()
end.time - start.time

# 43 minutes
```

```{r ntree_tuning_results}
# load ntree tuning model list
modellist <- readRDS("output/modelList_ntree_tuning.rds")

#Compare results
results <- resamples(modellist)
summary(results)

dotplot(results)
```





## Classify species 

We classified species using the optimal values for mtry (27) and ntree (1000)

```{r randomForest_species_train, eval=FALSE}
set.seed(44)

start.time = Sys.time()

# set up parallel 
cl <- makePSOCKcluster(parallel::detectCores() -1) 
registerDoParallel(cl)

# CARET train RF model using the optimal parameters 
#rf_classifier_species <- train(factor(genusSpecies) ~ ., 
#                              data = train_data, 
#                              method = "rf",
#                              ntree = 1000,
#                              tuneGrid=expand.grid(mtry = 27),
#                              metric = "Accuracy")

# RANDOMFOREST train RF model using optimal parameters
rf_classifier_species <- randomForest::randomForest(factor(genusSpecies) ~ ., 
                                           data=train_data, 
                                           importance=TRUE,
                                            mtry = 27,
                                            ntree = 1000)

# when you are done with parallel
stopCluster(cl)

end.time = Sys.time()
end.time - start.time

# show classification results
print(rf_classifier_species)

# save classifier to file 
saveRDS(object = rf_classifier_species, file = "output/rf_classifier_species_mtry27_ntree100.rds")
```

```{r load_rf}
rf_classifier_species <- readRDS("output/rf_classifier_species_mtry27_ntree100.rds")
```

```{r randomForest_species_valid}
# predict species using independent validation set
rf_valid_preds_species <- predict(rf_classifier_species, valid_data, type="class")
```

### Overall Accuracy 

```{r randomForest_OA}
# combine observed and predicted values into a single data frame
rf_train_results <- tibble(obs = as.character(train_data$genusSpecies),
                           pred = as.character(as.vector(rf_classifier_species$pred)))

rf_valid_results <- tibble(obs = as.character(valid_data$genusSpecies),
                              pred = as.character(as.vector(rf_valid_preds_species)))

# calculate overall accuracy for training set predictions 
OA_randomForest_train <- sum(rf_train_results$obs == rf_train_results$pred) / nrow(rf_train_results)
print(paste("Overall Accuracy of TRAINING set SPECIES predictions:", round(OA_randomForest_train, 4)))

# calculate overall accuracy for validation set predictions 
OA_randomForest_valid <- sum(rf_valid_results$obs == rf_valid_results$pred) / nrow(rf_valid_results)
print(paste("Overall Accuracy of VALIDATION set SPECIES predictions:", round(OA_randomForest_valid, 4)))
```

### Confusion Matrices

Visualize classification results across species for the TRAINING SET using ggplot

```{r randomForest_train_confMatrix, fig.width=14, fig.height=14, message=FALSE, eval=FALSE}
# list all combinations of true and predicted species 
all_combos <- expand.grid(obs = unique(rf_train_results$obs), 
				    pred = unique(rf_train_results$obs))

# format the observed and predicted values into a conf matrix

rf_train_results %>% 
  # count number of each obs/pred combination
  count(obs,pred) %>% 
  # fill in combinations with counts of zero
  complete(obs, pred, fill = list(n = 0)) %>% 
  # add rows witth species combinations not in predicted set
  right_join(all_combos) %>% 
  # set NA values to 0 in the n column
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  # set raster cell fill color based on count
  ggplot(aes(x = obs, y = fct_rev(pred), fill = n)) + 
  geom_raster() + 
  # add boxes around diagonal cells
  geom_tile(aes(color = obs == pred, width = 0.95, height = 0.95), size = 0.5) + 
  scale_color_manual(values = c(NA, "black")) + 
  # rotate x axis text 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Truth", y = "Predicted", title = "RandomForest Classification Confusion Matrix: Predicted vs. True species for Training Set") + 
  scale_fill_gradient(low = "white", high = "#43a2ca") + 
  # add count in each cell
  geom_text(aes(label = n)) + 
  # remove the color legend
  guides(color = "none")
```

Use the randomForest model to predict validation set species. Visualize classification results across species for the VALIDATION SET using ggplot: 

```{r randomForest_valid_confMatrix, fig.width=14, fig.height=14, message=FALSE}

# list all combinations of true and predicted species 
all_combos <- expand.grid(obs = unique(rf_valid_results$obs), 
				    pred = unique(rf_valid_results$obs))

# format the observed and predicted values into a conf matrix

rf_valid_results %>% 
  # count number of each obs/pred combination
  count(obs,pred) %>% 
  # fill in combinations with counts of zero
  complete(obs, pred, fill = list(n = 0)) %>% 
  # add rows witth species combinations not in predicted set
  right_join(all_combos) %>% 
  # set NA values to 0 in the n column
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  # set raster cell fill color based on count
  ggplot(aes(x = obs, y = fct_rev(pred), fill = n)) + 
  geom_raster() + 
  # add boxes around diagonal cells
  geom_tile(aes(color = obs == pred, width = 0.95, height = 0.95), size = 0.5) + 
  scale_color_manual(values = c(NA, "black")) + 
  # rotate x axis text 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Truth", y = "Predicted", title = "RandomForest Classification Confusion Matrix: Predicted vs. True SPECIES for Validation Set") + 
  scale_fill_gradient(low = "white", high = "#43a2ca") + 
  # add count in each cell
  geom_text(aes(label = n)) + 
  # remove the color legend
  guides(color = "none")


```

### Variable Importance 

```{r varImp_rf_species, results="hide"}

# format the randomForest variable importance for ggplot
imp <- as.data.frame(varImpPlot(rf_classifier_species))
imp$varnames <- rownames(imp) # row names to column
rownames(imp) <- NULL  

# isolate the band#'s and wavelength values
band_wl_lut <- cleaned_data %>% select(band_idx, wavelength_nm) %>% 
  # keep only rows with unique combinations of band_idx and wavelength_nm
  distinct()

# add a new column with wavelength_nm values
imp <- merge(imp, band_wl_lut, by.x="varnames", by.y="band_idx")
```

```{r varImp_byWavelength_plot, fig.height=6, fig.width=10}
ggplot(imp, aes(x=wavelength_nm,  
                y=MeanDecreaseAccuracy)) + 
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseAccuracy), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDA)",
       title = "Variable Importance: Mean Decrease Accuracy (MDA) vs. Wavelength",
       fill = "MDA")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#43a2ca")
```

```{r varImp_Gini, fig.height=6, fig.width=10}
ggplot(imp, aes(x = wavelength_nm,  
                y = MeanDecreaseGini)) + 
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseGini), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDG)",
       title = "Variable Importance: Mean Decrease Gini (MDG) vs. Wavelength",
       fill = "MDG")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#43a2ca")
```

### Add wavelengths of interest

For interpretation of the variable importance plots, wavelengths of interest for the red, green, blue, and NIR bands are added here. The wavelength ranges are based on [Landsat 8 designations](https://www.neonscience.org/hyper-spec-intro). 

```{r vertical_dashed_lines}
# plot variable importance with wavelengths of interest marked by dashed vertical lines 

ggplot(imp, aes(x = wavelength_nm,  
                y = MeanDecreaseGini)) + 
  
  # add vertical dashed lines to indicate notable wavelengths
  geom_vline(xintercept=480, linetype="dashed", color="blue", size=1, alpha=0.5) + # BLUE 
  geom_vline(xintercept=550, linetype="dashed", color="green", size=1, alpha=0.5) + # GREEN
  geom_vline(xintercept=650, linetype="dashed", color="red", size=1, alpha=0.5) + # RED
  geom_vline(xintercept=870, linetype="dashed", color="grey30", size=1, alpha=0.5) + # NIR 
  geom_vline(xintercept=1600, linetype="dashed", color="chocolate4", size=1, alpha=0.5) + # SWIR
  
  
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseGini), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDG)",
       title = "Variable Importance: Mean Decrease Gini (MDG) vs. Wavelength",
       fill = "MDG")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#43a2ca") 


```
```{r vertical_shaded_regions}
# plot variable importance with wavelengths of interest marked by dashed vertical lines 

ggplot(imp, aes(x = wavelength_nm,  
                y = MeanDecreaseGini)) + 
  
  # add vertical shaded regions to indicate notable wavelengths
  annotate(geom = "rect", xmin = 450, xmax = 510, ymin = -5, ymax = Inf, fill = "blue", alpha = 0.1) + # BLUE
  annotate(geom = "rect", xmin = 530, xmax = 590, ymin = -5, ymax = Inf, fill = "green", alpha = 0.1) + # GREEN
  annotate(geom = "rect", xmin = 640, xmax = 670, ymin = -5, ymax = Inf, fill = "red", alpha = 0.1) + # RED
  annotate(geom = "rect", xmin = 850, xmax = 880, ymin = -5, ymax = Inf, fill = "grey30", alpha = 0.1)  + # NIR 
  annotate(geom = "rect", xmin = 1570, xmax = 1650, ymin = -5, ymax = Inf, fill = "chocolate4", alpha = 0.1)  + # SWIR
  
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseGini), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDG)",
       title = "Variable Importance: Mean Decrease Gini (MDG) vs. Wavelength",
       fill = "MDG")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#43a2ca") 
  

```




## Classify genus


```{r set_up_data_genus}
# columns to keep for training the classifier
cols_to_keep_genus <- c("spectraID", "band_idx", "reflectance", "genus")

# training set -------------------------------------------------
# select and format the training data 
train_data_genus <- cleaned_data %>% 
  # keep only the spectra marked as part of the "train" or "test" set
    # train 60% + test 20% = 80% for training with k-fold cross validation
  dplyr::filter(group == "train" | group == "test") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep_genus)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 

# convert label to factor, so R does classification instead of regression
train_data_genus$genus <- factor(train_data_genus$genus)



# validation set -----------------------------------------------
valid_data_genus <- cleaned_data %>% 
  #  keep only the spectra marked as part of the independent validation set
  dplyr::filter(group == "valid") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep_genus)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 

valid_data_genus$genus <- factor(valid_data_genus$genus)
```

```{r randomForest_train_genus, eval=FALSE}
set.seed(44) # for obama

start.time = Sys.time()

# set up parallel 
cl <- makePSOCKcluster(parallel::detectCores() -1) 
registerDoParallel(cl)

# train random forest classifiers: predict genus
rf_classifier_genus <- randomForest::randomForest(factor(genus) ~ ., 
                                           data=train_data_genus, 
                                           importance=TRUE,
                                           mtry=27,
                                           ntree=1000)
# when you are done with parallel
stopCluster(cl)
end.time = Sys.time()
end.time - start.time

# show classification results
print(print(rf_classifier_genus))

# save classifier to file 
saveRDS(object = rf_classifier_genus, file = "output/rf_classifier_genus_mtry27_ntree100.rds")
```

```{r}
rf_classifier_genus <- readRDS("output/rf_classifier_genus_mtry27_ntree100.rds")
```

### Overall Accuracy 

Predict genus for the validation set and assess overall accuracy for the training and validation sets. 

```{r randomForest_OA_genus}
# predict species for validation set
p_genus <- predict(rf_classifier_genus, valid_data_genus, type = "class")

# combine observed and predicted values into a single data frame
rf_train_results_genus <- tibble(obs = as.character(train_data_genus$genus),
                              pred = as.character(as.vector(rf_classifier_genus$predicted)))

rf_valid_results_genus <- tibble(obs = as.character(valid_data_genus$genus),
                              pred = as.character(as.vector(p_genus)))

# calculate overall accuracy for training set predictions 
OA_randomForest_train_genus <- sum(rf_train_results_genus$obs == rf_train_results_genus$pred) / nrow(rf_train_results_genus)
print(paste("Overall Accuracy of training set genus predictions:", round(OA_randomForest_train_genus, 4)))

# calculate overall accuracy for validation set predictions 
OA_randomForest_valid_genus <- sum(rf_valid_results_genus$obs == rf_valid_results_genus$pred) / nrow(rf_valid_results)
print(paste("Overall Accuracy of validation set genus predictions:", round(OA_randomForest_valid_genus, 4)))
```

### Confusion Matrices

```{r randomForest_valid_confMatrix_genus, fig.width=12, fig.height=12, message=FALSE}

# list all combinations of true and predicted species 
all_combos_genus <- expand.grid(obs = unique(rf_valid_results_genus$obs), 
				    pred = unique(rf_valid_results_genus$obs))

# format the observed and predicted values into a conf matrix

rf_valid_results_genus %>% 
  # count number of each obs/pred combination
  count(obs,pred) %>% 
  # fill in combinations with counts of zero
  complete(obs, pred, fill = list(n = 0)) %>% 
  # add rows witth species combinations not in predicted set
  right_join(all_combos_genus) %>% 
  # set NA values to 0 in the n column
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  # set raster cell fill color based on count
  ggplot(aes(x = obs, y = fct_rev(pred), fill = n)) + 
  geom_raster() + 
  # add boxes around diagonal cells
  geom_tile(aes(color = obs == pred, width = 0.95, height = 0.95), size = 0.5) + 
  scale_color_manual(values = c(NA, "black")) + 
  # rotate x axis text 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Truth", y = "Predicted", title = "RandomForest Classification Confusion Matrix: Predicted vs. True GENUS for Validation Set") + 
  scale_fill_gradient(low = "white", high = "#fc8d59") + 
  # add count in each cell
  geom_text(aes(label = n)) + 
  # remove the color legend
  guides(color = "none")
```


```{r randomForest_varImpPlot_ggplot_genus, results="hide"}
imp_genus <- as.data.frame(varImpPlot(rf_classifier_genus))
imp_genus$varnames <- rownames(imp_genus) # row names to column
rownames(imp_genus) <- NULL  
# add a new column with wavelength_nm values
imp_genus <- merge(imp_genus, band_wl_lut, by.x="varnames", by.y="band_idx")
```

```{r varImp_MDA_genus, fig.height=6, fig.width=10}
ggplot(imp_genus, aes(x=wavelength_nm,  
                y=MeanDecreaseAccuracy)) + 
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseAccuracy), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDA)",
       title = "GENUS classification variable importance: Mean Decrease Accuracy (MDA) vs. Wavelength",
       fill = "MDA")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#fc8d59")
```


```{r varImp_Gini_genus, fig.height=6, fig.width=10}
ggplot(imp_genus, aes(x = wavelength_nm,  
                y = MeanDecreaseGini)) + 
  # add bars where each height is equal to the variable importance metric
  geom_col(aes(fill = MeanDecreaseGini), color = NA) +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", 
       y = "Variable importance (MDG)",
       title = "GENUS classification variable importance: Mean Decrease Gini (MDG) vs. Wavelength",
       fill = "MDG")  + 
  scale_fill_gradient(low = "#d9d9d9", high = "#fc8d59")
```



### Add reflectance spectrum to interpret variable importance? 


```{r varImp_with_refl_spectrum}
imp_and_refl <- imp

```

```{r randomForest_varImpPlot_ggplot, eval=FALSE, include=FALSE}

# default varImpPlot visualization in randomForest
#varImpPlot(rf_classifier)

# format the randomForest variable importance for ggplot
imp <- as.data.frame(varImpPlot(rf_classifier))
imp$varnames <- rownames(imp) # row names to column
rownames(imp) <- NULL  

# keep the n features with the highest MeanDecreaseAccuracy values 
imp_top30_mda <- arrange(imp, desc(MeanDecreaseAccuracy)) %>% 
  select(c(MeanDecreaseAccuracy, varnames)) %>% 
  slice_head(n = 40)


ggplot(imp_top30_mda, aes(x=reorder(varnames, MeanDecreaseAccuracy), 
                y=MeanDecreaseAccuracy
                #,color=MeanDecreaseAccuracy
                )) + 
  geom_point() +
  geom_segment(aes(x=varnames,xend=varnames,y=0,yend=MeanDecreaseAccuracy)) +
  #scale_color_continuous(type = "viridis") +
  ylab("MDA") +
  xlab("Variable Name") +
  ggtitle("RandomForest variable importance: Mean Decrease Accuracy (MDA)") +
  labs(color = "MDA") + 
  coord_flip() + 
  theme_minimal()
```



```{r how_long_did_this_take}
# stop timer
end_time <- Sys.time()

elapsed_time <- end_time - start_time

print(difftime(end_time, start_time, 
         units = c("auto", "secs", "mins", "hours",
                   "days", "weeks")))
```

