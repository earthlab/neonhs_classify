---
title: "Classify veg"
author: "Victoria Scholl"
date: "08/06/2020 Earth Lab GRA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidyr)
library(dplyr)
library(randomForest)
library(caret)
library(rfPermute)
```

## Classify species

```{r read_data}
# read the cleaned data, labeled for train, test, 
cleaned_data <- read_csv(here::here("data","cleaned_spectra.csv"))
```

Format the data so each row is a sample (reflectance spectrum) and each column is a descriptive feature (reflectance value at each wavelength). Each row must have a label to predict for the classification, such as genusSpecies (the first two pieces of each scientific name excluding any variety info. Example: Pseudotsuga menziesii. Genus is Pseudotsuga, species is menziesii).

```{r format_data}
# columns to keep for training the classifier
cols_to_keep <- c("spectraID", "band_idx", "reflectance", "genusSpecies")

# training set -------------------------------------------------
# select and format the training data 
train_data <- cleaned_data %>% 
  # keep only the spectra marked as part of the "train" or "test" set
    # train 60% + test 20% = 80% for training with k-fold cross validation
  dplyr::filter(group == "train" | group == "test") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # filter spectra identified at the genus level, indicated by species == "sp."
  dplyr::filter(species != "sp.") %>% 
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 
  # VS-NOTE: subset small # of samples for testing!!!
  #dplyr::sample_frac(0.5)

# convert genusSpecies to factor, so R does classification instead of regression.
# reassign the factor levels after subsetting the training data
train_data$genusSpecies <- factor(train_data$genusSpecies)



# validation set -----------------------------------------------
valid_data <- cleaned_data %>% 
  #  keep only the spectra marked as part of the independent validation set
  dplyr::filter(group == "valid") %>% 
  # remove all entries with NA reflectance values, indicative of bad bands
  dplyr::filter(!is.na(reflectance)) %>% 
  # filter spectra identified at the genus level, indicated by species == "sp."
  dplyr::filter(species != "sp.") %>%
  # keep only the columns needed for classification
  dplyr::select(all_of(cols_to_keep)) %>%
  # reshape the data from long to wide so each row is a spectrum, 
    # and each col contains reflectance per band.
  tidyr::pivot_wider(names_from = band_idx, 
                     values_from = reflectance) %>% 
  # remove spectraID since it's not a descriptive feature 
  dplyr::select(-spectraID) 

valid_data$genusSpecies <- factor(valid_data$genusSpecies)
```

```{r randomForest_train}
set.seed(44)

# train random forest classifiers: predict genusSpecies
rf_classifier <- randomForest::randomForest(factor(genusSpecies) ~ ., 
                                           data=train_data, 
                                           importance=TRUE
                                           #,do.trace=TRUE
                                           )

print(rf_classifier)
```

### Confusion matrices

```{r formatted_conf_matrix, fig.height=12, fig.width=12}
# nicely formatted confusion matrix using the rfPermute library
#confMatrix <- rfPermute::plotConfMat(rf = rf_classifier, 
#                                     title = "randomForest confusion matrix: training set") 

#confMatrix
```


```{r randomForest_valid, fig.width=12, fig.height=12}

# predict species for validation set
p <- predict(rf_classifier, valid_data, type = "class")

# combine observed and predicted values into a single data frame
rf_valid_results <- tibble(obs = as.character(valid_data$genusSpecies),
                              pred = as.character(as.vector(p)))

all_combos <- expand.grid(obs = unique(rf_valid_results$obs), 
				    pred = unique(rf_valid_results$obs))

# format the observed and predicted values into a conf matrix

rf_valid_results %>% 
  # count number of each obs/pred combination
  count(obs,pred) %>% 
  # fill in combinations with counts of zero
  complete(obs, pred, fill = list(n = 0)) %>% 
  # add rows witth species combinations not in predicted set
  right_join(all_combos) %>% 
  # set NA values to 0 in the n column
  mutate(n = ifelse(is.na(n), 0, n)) %>% 
  # set raster cell fill color based on count
  ggplot(aes(x = obs, y = fct_rev(pred), fill = n)) + 
  geom_raster() + 
  # add boxes around diagonal cells
  geom_tile(aes(color = obs == pred), size = 0.5) + 
  scale_color_manual(values = c(NA, "black")) + 
  # rotate x axis text 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Truth", y = "Predicted") + 
  scale_fill_gradient(low = "white", high = "blue") + 
  # add count in each cell
  geom_text(aes(label = n)) + 
  # remove the color legend
  guides(color = "none")

# VS-NOTE: adjust transparency of the geom_tile lines? 

```

### Variable importance 

```{r randomForest_varImpPlot_default, fig.height=6, fig.width=5}
# default varImpPlot visualization in randomForest
varImpPlot(rf_classifier)
```
```{r randomForest_varImpPlot_ggplot, fig.height=8, fig.width=6}

# format the randomForest variable importance for ggplot
imp <- as.data.frame(varImpPlot(rf_classifier))
imp$varnames <- rownames(imp) # row names to column
rownames(imp) <- NULL  

# keep the n features with the highest MeanDecreaseAccuracy values 
imp_top30_mda <- arrange(imp, desc(MeanDecreaseAccuracy)) %>% 
  select(c(MeanDecreaseAccuracy, varnames)) %>% 
  slice_head(n = 40)


ggplot(imp_top30_mda, aes(x=reorder(varnames, MeanDecreaseAccuracy), 
                y=MeanDecreaseAccuracy
                #,color=MeanDecreaseAccuracy
                )) + 
  geom_point() +
  geom_segment(aes(x=varnames,xend=varnames,y=0,yend=MeanDecreaseAccuracy)) +
  #scale_color_continuous(type = "viridis") +
  ylab("MDA") +
  xlab("Variable Name") +
  ggtitle("RandomForest variable importance: Mean Decrease Accuracy (MDA)") +
  labs(color = "MDA") + 
  coord_flip() + 
  theme_minimal()
```
```{r varImp_byWavelength}
# plot variable importance as a function of ordered wavelength bands 

# isolate the band#'s and wavelength values
band_wl_lut <- cleaned_data %>% select(band_idx, wavelength_nm) %>% 
  # keep only rows with unique combinations of band_idx and wavelength_nm
  distinct()

# add a new column with wavelength_nm values
imp <- merge(imp, band_wl_lut, by.x="varnames", by.y="band_idx")

```

```{r varImp_byWavelength_plot, fig.height=4, fig.width=7}
ggplot(imp, aes(x=wavelength_nm,  
                y=MeanDecreaseAccuracy)) + 
  # add bars where each height is equal to the variable importance metric
  geom_col() +
  theme_minimal() + 
  labs(x = "Wavelength (nm)", y = "Variable importance (Mean Decrease in Accuracy)")  
```


```{r caret}
# caret = Classification And REgression Training 

# tuning parameters to try: ntree, mtry
tunegrid <- expand.grid(mtry = seq(from = 3, to = 24, by = 3))

#10 folds repeat 3 times --> alternative to the train/test/valid, dont need both
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = "grid")

start_time = Sys.time()

rf_gridsearch <- caret::train(factor(genusSpecies) ~ ., 
                data = train_data, 
                method = "rf",
                metric = "Accuracy",
                tuneGrid=tunegrid, 
                trControl=control,
                verbose = TRUE)

end_time = Sys.time()

end_time - start_time

# Print the results
print(rf_gridsearch)
plot(rf_gridsearch)
```

```{r}
# https://rpubs.com/phamdinhkhanh/389752

set.seed(44)
# iteratively check ntree values 
for (ntree in c(100,500,1000)){
  print(ntree)
  fit <- train(caret::train(factor(genusSpecies) ~ ., 
                data = train_data, 
                method = "rf",
                metric = "Accuracy",
                ntree = ntree,
                tuneGrid=expand.grid(mtry = 21), 
                trControl=control,
                verbose = TRUE))
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Compare results
results <- resamples(modellist)
summary(results)

dotplot(results)
```
```{r}
tunegrid <- expand.grid(mtry = seq(from = 3, to = 24, by = 3))

tuneGrid <- expand.grid(mtry = 21)

rf_mtry_ntree <- caret::train(factor(genusSpecies) ~ ., 
                data = train_data, 
                method = "rf",
                metric = "Accuracy",
                ntree = 100,
                tuneGrid=tunegrid,
                verbose = TRUE)
```

```{r}
# caret ranger? 
# https://stackoverflow.com/questions/48334929/r-using-ranger-with-caret-tunegrid-argument 
# parallel
# k-fold cross val (merge train+valid, keep separate test set) OR train/test/valid split
# test ntree AND mtry values 
# conf matrix + var imp plot formatted
```

```{r}
#https://afit-r.github.io/random_forests#tune


```

